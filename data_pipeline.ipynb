{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51047, 58)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/raw/cell2celltrain.csv\")\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    snake_case = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return snake_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [camel_to_snake(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('not_new_cellphone_user', axis=1)\n",
    "df = df.drop('service_area', axis=1)\n",
    "df = df.drop('customer_id',axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Features\n",
    "# TotalCareIssues=CustomerCareCalls+BlockedCalls+DroppedBlockedCalls\n",
    "df['total_care_issues'] = df['customer_care_calls'] + df['blocked_calls'] + df['dropped_blocked_calls']\n",
    "df = df.drop(['customer_care_calls', 'blocked_calls', 'dropped_blocked_calls'], axis=1)\n",
    "\n",
    "# OverallRevenue=MonthlyRevenue+TotalRecurringCharge\n",
    "df['overall_revenue'] = df['monthly_revenue'] + df['total_recurring_charge']\n",
    "df = df.drop(['monthly_revenue', 'total_recurring_charge'], axis=1)\n",
    "\n",
    "# OverallUsage=MonthlyMinutes + OverageMinutes + RoamingCalls\n",
    "df['overall_usage'] = df['monthly_minutes'] + df['overage_minutes'] + df['roaming_calls']\n",
    "df = df.drop(['monthly_minutes', 'overage_minutes', 'roaming_calls'], axis=1)\n",
    "\n",
    "# ValueAddedServiceUsage=DirectorAssistedCalls + ThreewayCalls + CallForwardingCalls + CallWaitingCalls\n",
    "df['value_added_service_usage'] = df['director_assisted_calls'] + df['threeway_calls'] + df['call_forwarding_calls'] + df['call_waiting_calls']\n",
    "df = df.drop(['director_assisted_calls', 'threeway_calls', 'call_forwarding_calls', 'call_waiting_calls'], axis=1)\n",
    "\n",
    "#TotalCalls=InboundCalls+OutboundCalls\n",
    "df['total_calls'] = df['inbound_calls'] + df['outbound_calls']\n",
    "df = df.drop(['inbound_calls', 'outbound_calls'], axis=1)\n",
    "\n",
    "#TotalPeakoffPeakcall =PeakCallsInOut+OffPeakCallsInOut\n",
    "df['total_peak_off_peak_calls'] = df['peak_calls_in_out'] + df['off_peak_calls_in_out']\n",
    "df = df.drop(['peak_calls_in_out', 'off_peak_calls_in_out'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_categories(df, verbose=False):\n",
    "    quantitative_columns = df.select_dtypes(include=['float64']).columns\n",
    "    quantitative_discrete_columns = df.select_dtypes(include=['int64']).columns\n",
    "    id_columns = ['customer_id']\n",
    "    quantitative_discrete_columns = [col for col in quantitative_discrete_columns if col not in id_columns]\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    binary_columns = [col for col in categorical_columns if df[col].nunique() == 2]\n",
    "    categorical_columns = [col for col in categorical_columns if col not in binary_columns]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nQuantitative Variables:\", list(quantitative_columns))\n",
    "        print(\"Total Quantitative-Continuous Variables:\", len(quantitative_columns))\n",
    "\n",
    "        print(\"\\nQuantitative Discrete Variables:\", list(quantitative_discrete_columns))\n",
    "        print(\"Total Quantitative-Discrete Variables:\", len(quantitative_discrete_columns))\n",
    "\n",
    "        print(\"\\nCategorical Variables:\", list(categorical_columns))\n",
    "        print(\"Total Categorical Variables:\", len(categorical_columns))\n",
    "\n",
    "        print(\"\\nBinary Variables:\", list(binary_columns))\n",
    "        print(\"Total Binary Variables:\", len(binary_columns))\n",
    "\n",
    "    return quantitative_columns, quantitative_discrete_columns, categorical_columns, binary_columns, id_columns\n",
    "\n",
    "quantitative_columns, quantitative_discrete_columns, categorical_columns, binary_columns, id_columns = get_data_categories(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Variable Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binary_columns:\n",
    "    if col == \"homeownership\":\n",
    "        df[col] = df[col].map({\"Known\": 1, \"Unknown\": 0})\n",
    "    else:\n",
    "        df[col] = df[col].map({\"Yes\": 1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Variables Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To prevent data leakage we split before applying imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, train_size = 0.80, random_state = random_state)\n",
    "\n",
    "# X_train = train.drop('churn', axis=1).reset_index(drop=True)\n",
    "# y_train = train['churn'].reset_index(drop=True)\n",
    "\n",
    "# X_test = test.drop('churn', axis=1).reset_index(drop=True)\n",
    "# y_test = test['churn'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_regression_imputer(df, column):\n",
    "\n",
    "    df_known = df[df[column].notna()]\n",
    "    df_missing = df[df[column].isna()]\n",
    "\n",
    "    # Select features for the model\n",
    "    features = df.columns.difference([column])\n",
    "\n",
    "    # Define training data and replace NaNs with 0\n",
    "    X_known = df_known[features].fillna(0)\n",
    "    y_known = df_known[column].fillna(0)\n",
    "\n",
    "    # Train a Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_known, y_known)\n",
    "\n",
    "    # Predict missing values for this column\n",
    "    X_missing = df_missing[features].fillna(0)  # Replace NaNs with 0 in missing data\n",
    "\n",
    "    predicted_values = model.predict(X_missing)\n",
    "    random_noise = np.random.normal(\n",
    "        loc=0, scale=y_known.std(), size=predicted_values.shape\n",
    "    )\n",
    "    # only positive values\n",
    "    df.loc[df[column].isna(), column] = np.max(predicted_values + random_noise, 0)\n",
    "    return df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute train\n",
    "quant_df = train[quantitative_columns].copy()\n",
    "for col in quant_df.columns:\n",
    "    if train[col].isna().sum() > 0:\n",
    "        train[col] = random_regression_imputer(quant_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute test\n",
    "quant_df = test[quantitative_columns].copy()\n",
    "for col in quant_df.columns:\n",
    "    if test[col].isna().sum() > 0:\n",
    "        test[col] = random_regression_imputer(quant_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_outliers_iqr_multiple_columns(df: pd.DataFrame, columns:str, threshold: float, verbose=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Trim outliers from multiple DataFrame columns using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        columns (list): A list of column names from which to trim outliers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if verbose:\n",
    "            print(f\"Processing column: {column}\")\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        if verbose: \n",
    "            print(\"Dimensions before:\", df.shape)\n",
    "        # Create a mask for values within bounds\n",
    "        mask = (df[column] >= lower_bound) & (df[column] <= upper_bound)\n",
    "        \n",
    "        # Apply the mask to keep only the non-outliers\n",
    "        df = df[mask]\n",
    "        if verbose:\n",
    "            print(\"Dimensions after:\", df.shape)\n",
    "        \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trim_outliers_iqr_multiple_columns(\n",
    "    train, \n",
    "    quantitative_columns,\n",
    "    threshold=3\n",
    ")\n",
    "test = trim_outliers_iqr_multiple_columns(\n",
    "    test, \n",
    "    quantitative_columns,\n",
    "    threshold=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_numeric(df, verbose=False):\n",
    "    # handset_price\n",
    "    # credit_rating\n",
    "\n",
    "    df['handset_price'] = df['handset_price'].replace(\"Unknown\", np.nan).astype(float)\n",
    "    df['handset_price'] = df['handset_price'].fillna(df['handset_price'].mean())\n",
    "\n",
    "    df['credit_rating'] = df['credit_rating'].str.split('-').str[0].str.strip()\n",
    "    df['credit_rating'] = df['credit_rating'].astype(int)\n",
    "\n",
    "    if verbose:\n",
    "        print (df[categorical_columns].isna().sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = categorical_to_numeric(train)\n",
    "test = categorical_to_numeric(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_encoding(df, columns):\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
    "    encoder.fit(df[columns])\n",
    "\n",
    "    train_encoded = encoder.transform(df[columns])\n",
    "    train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "    df = pd.concat([df.drop(columns, axis=1), train_encoded_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverge the Pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_vars = [\n",
    "    \"prizm_code\",\n",
    "    \"occupation\",\n",
    "    \"marital_status\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe = ohe_encoding(train, ohe_vars).copy()\n",
    "test_ohe = ohe_encoding(test, ohe_vars).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ohe_var in ohe_vars:\n",
    "    train[ohe_var] = train[ohe_var].astype('category')\n",
    "    test[ohe_var] = test[ohe_var].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Inflated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get zero inflated features\n",
    "def get_zero_inflated_features(df, threshold=0.5):\n",
    "    zero_inflated = []\n",
    "    for col in df.columns:\n",
    "        if df[col].value_counts().get(0) is None:\n",
    "            continue\n",
    "\n",
    "        if df[col].value_counts().get(0) / len(df) > threshold:\n",
    "            zero_inflated.append(col)\n",
    "    return zero_inflated\n",
    "\n",
    "def make_zero_inflated_indicators(df):\n",
    "    nonbinary_feature_names = list(quantitative_discrete_columns) + list(quantitative_columns)\n",
    "\n",
    "    zero_inflated_features = get_zero_inflated_features(df[nonbinary_feature_names], threshold=0.1)\n",
    "\n",
    "    for feature in zero_inflated_features:\n",
    "        df[f'{feature}_is_zero'] = (df[feature] == 0).astype(int)\n",
    "    return df\n",
    "\n",
    "df = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "df = make_zero_inflated_indicators(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = pd.concat([train_ohe, test_ohe], axis=0).reset_index(drop=True)\n",
    "df_ohe = make_zero_inflated_indicators(df_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, train_size = 0.80, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('churn', axis=1).reset_index(drop=True)\n",
    "y_train = train['churn'].reset_index(drop=True)\n",
    "\n",
    "X_test = test.drop('churn', axis=1).reset_index(drop=True)\n",
    "y_test = test['churn'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.isna().sum().sum() == 0\n",
    "assert X_test.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.isna().sum().sum() == 0\n",
    "assert y_test.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/prod/X_train_cat.csv\", index=False)\n",
    "y_train.to_csv(\"./data/prod/y_train_cat.csv\", index=False)\n",
    "\n",
    "X_test.to_csv(\"./data/prod/X_test_cat.csv\", index=False)\n",
    "y_test.to_csv(\"./data/prod/y_test_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe, test_ohe = train_test_split(df_ohe, train_size = 0.80, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_ohe.drop(\"churn\", axis=1)\n",
    "y_train = train_ohe[\"churn\"].reset_index(drop=True)\n",
    "\n",
    "X_test = test_ohe.drop(\"churn\", axis=1)\n",
    "y_test = test_ohe[\"churn\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.isna().sum().sum() == 0\n",
    "assert X_test.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.isna().sum().sum() == 0\n",
    "assert y_test.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/prod/X_train_ohe.csv\", index=False)\n",
    "y_train.to_csv(\"./data/prod/y_train_ohe.csv\", index=False)\n",
    "\n",
    "X_test.to_csv(\"./data/prod/X_test_ohe.csv\", index=False)\n",
    "y_test.to_csv(\"./data/prod/y_test_ohe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
